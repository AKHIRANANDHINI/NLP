{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPL5FUgJnEfW+tm0L/P4ATo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AKHIRANANDHINI/NLP/blob/main/Lab%205.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import zipfile\n",
        "nltk.download('stopwords')\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "MAX_NUM_WORDS = 30000\n",
        "MAX_SEQUENCE_LENGTH = 40\n",
        "EMBEDDING_DIM = 100\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 8\n",
        "VALIDATION_SPLIT = 0.1\n",
        "\n",
        "# Correctly read 'Tweets.csv' from the zip file\n",
        "with zipfile.ZipFile(\"/content/archive (1).zip\") as z:\n",
        "    with z.open(\"Tweets.csv\") as f:\n",
        "        df = pd.read_csv(f)\n",
        "\n",
        "df = df.rename(columns={'airline_sentiment': 'target'}) # Rename for consistency\n",
        "df = df[['text','target']].dropna().reset_index(drop=True)\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def clean_tweet(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'@\\w+', ' ', text)\n",
        "    text = re.sub(r'http\\S+|www.\\S+', ' ', text)\n",
        "    text = re.sub(r'#\\w+', ' ', text)\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text) # Fix: removed newline character\n",
        "    tokens = text.split() # Fix: define tokens here\n",
        "    tokens = [t for t in tokens if t not in STOPWORDS and len(t)>1]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "tqdm.pandas()\n",
        "df['clean_text'] = df['text'].progress_apply(clean_tweet)\n",
        "\n",
        "X = df['clean_text'].values\n",
        "y = df['target'].values\n",
        "\n",
        "# Convert target to numerical if it's categorical (e.g., 'positive', 'negative', 'neutral')\n",
        "# Assuming the goal is binary classification, let's map 'positive' to 1 and 'negative' to 0, and remove 'neutral'\n",
        "# If the problem requires multi-class, this needs adjustment.\n",
        "# For now, let's filter for binary classification as implied by binary metrics and sigmoid output\n",
        "df_binary = df[df['target'].isin(['positive', 'negative'])].copy()\n",
        "df_binary['target'] = df_binary['target'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "X = df_binary['clean_text'].values\n",
        "y = df_binary['target'].values\n",
        "\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)\n",
        "\n",
        "cv = CountVectorizer(max_features=20000, ngram_range=(1,2))\n",
        "tfidf = TfidfVectorizer(max_features=30000, ngram_range=(1,2))\n",
        "X_train_cv = cv.fit_transform(X_train_raw)\n",
        "X_test_cv = cv.transform(X_test_raw)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_raw)\n",
        "X_test_tfidf = tfidf.transform(X_test_raw)\n",
        "\n",
        "def eval_and_print(model, X_test, y_test, name):\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', zero_division=0)\n",
        "    print(f\"{name} -> Acc: {acc:.4f}, Precision: {p:.4f}, Recall: {r:.4f}, F1: {f1:.4f}\")\n",
        "    return {'model': name, 'accuracy': acc, 'precision': p, 'recall': r, 'f1': f1}\n",
        "\n",
        "results = []\n",
        "lr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_SEED)\n",
        "lr.fit(X_train_tfidf, y_train)\n",
        "results.append(eval_and_print(lr, X_test_tfidf, y_test, \"LogisticRegression-TFIDF\"))\n",
        "\n",
        "svm = LinearSVC(max_iter=2000, class_weight='balanced', random_state=RANDOM_SEED)\n",
        "svm.fit(X_train_tfidf, y_train)\n",
        "results.append(eval_and_print(svm, X_test_tfidf, y_test, \"LinearSVC-TFIDF\"))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token='')\n",
        "tokenizer.fit_on_texts(X_train_raw)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_raw)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test_raw)\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "def compute_metrics_from_probs(probs, y_true, threshold=0.5):\n",
        "    y_pred = (probs >= threshold).astype(int)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
        "    return acc, p, r, f1\n",
        "\n",
        "def train_and_evaluate_keras(model, X_tr, y_tr, X_te, y_te, name, epochs=EPOCHS, batch_size=BATCH_SIZE):\n",
        "    es = callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "    history = model.fit(X_tr, y_tr, validation_split=VALIDATION_SPLIT, epochs=epochs, batch_size=batch_size, class_weight=class_weight_dict, callbacks=[es], verbose=1)\n",
        "    probs = model.predict(X_te, batch_size=128).ravel()\n",
        "    acc, p, r, f1 = compute_metrics_from_probs(probs, y_te)\n",
        "    print(f\"{name} -> Acc: {acc:.4f}, Precision: {p:.4f}, Recall: {r:.4f}, F1: {f1:.4f}\")\n",
        "    return {'model': name, 'accuracy': acc, 'precision': p, 'recall': r, 'f1': f1, 'history': history}\n",
        "\n",
        "def build_mlp_avg(vocab_size, embedding_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH):\n",
        "    inp = layers.Input(shape=(input_length,))\n",
        "    x = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length)(inp)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    out = layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "mlp_model = build_mlp_avg(vocab_size=vocab_size)\n",
        "results.append(train_and_evaluate_keras(mlp_model, X_train_pad, y_train, X_test_pad, y_test, \"MLP-AverageEmb\"))\n",
        "\n",
        "def build_cnn1d(vocab_size, embedding_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH):\n",
        "    inp = layers.Input(shape=(input_length,))\n",
        "    x = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length)(inp)\n",
        "    convs = []\n",
        "    for fsz in [2,3,4]:\n",
        "        c = layers.Conv1D(filters=128, kernel_size=fsz, activation='relu')(x)\n",
        "        c = layers.GlobalMaxPooling1D()(c)\n",
        "        convs.append(c)\n",
        "    x = layers.concatenate(convs)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    out = layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "cnn_model = build_cnn1d(vocab_size=vocab_size)\n",
        "results.append(train_and_evaluate_keras(cnn_model, X_train_pad, y_train, X_test_pad, y_test, \"CNN1D\"))\n",
        "\n",
        "def build_lstm(vocab_size, embedding_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH):\n",
        "    inp = layers.Input(shape=(input_length,))\n",
        "    x = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length)(inp)\n",
        "    x = layers.SpatialDropout1D(0.2)(x)\n",
        "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=False))(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    out = layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "lstm_model = build_lstm(vocab_size=vocab_size)\n",
        "results.append(train_and_evaluate_keras(lstm_model, X_train_pad, y_train, X_test_pad, y_test, \"BiLSTM\"))\n",
        "\n",
        "print(\"\\n=== RESULTS SUMMARY ==\")\n",
        "res_df = pd.DataFrame(results)\n",
        "print(res_df[['model','accuracy','precision','recall','f1']].sort_values(by='f1', ascending=False).to_string(index=False))\n",
        "res_df.to_csv('model_comparison_results.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMrRHH23Bz-J",
        "outputId": "b6bf4ce8-a3a7-4d9d-f460-3a0697dead62"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "100%|██████████| 14640/14640 [00:00<00:00, 64845.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression-TFIDF -> Acc: 0.8982, Precision: 0.7439, Recall: 0.7674, F1: 0.7555\n",
            "LinearSVC-TFIDF -> Acc: 0.9078, Precision: 0.7826, Recall: 0.7611, F1: 0.7717\n",
            "Epoch 1/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.4780 - loss: 0.6796 - val_accuracy: 0.7846 - val_loss: 0.5406\n",
            "Epoch 2/8\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.7996 - loss: 0.4347 - val_accuracy: 0.8214 - val_loss: 0.4131\n",
            "Epoch 3/8\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8854 - loss: 0.2652 - val_accuracy: 0.8636 - val_loss: 0.3212\n",
            "Epoch 4/8\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9073 - loss: 0.2148 - val_accuracy: 0.8939 - val_loss: 0.2587\n",
            "Epoch 5/8\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9337 - loss: 0.1637 - val_accuracy: 0.8896 - val_loss: 0.2747\n",
            "Epoch 6/8\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9440 - loss: 0.1456 - val_accuracy: 0.8864 - val_loss: 0.2941\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "MLP-AverageEmb -> Acc: 0.8887, Precision: 0.6915, Recall: 0.8245, F1: 0.7522\n",
            "Epoch 1/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 52ms/step - accuracy: 0.7691 - loss: 0.5795 - val_accuracy: 0.9004 - val_loss: 0.2693\n",
            "Epoch 2/8\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 61ms/step - accuracy: 0.9083 - loss: 0.2376 - val_accuracy: 0.9004 - val_loss: 0.2660\n",
            "Epoch 3/8\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.9549 - loss: 0.1176 - val_accuracy: 0.9037 - val_loss: 0.3017\n",
            "Epoch 4/8\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.9774 - loss: 0.0602 - val_accuracy: 0.9015 - val_loss: 0.3609\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "CNN1D -> Acc: 0.8870, Precision: 0.6956, Recall: 0.7970, F1: 0.7429\n",
            "Epoch 1/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 178ms/step - accuracy: 0.7514 - loss: 0.5650 - val_accuracy: 0.8831 - val_loss: 0.2939\n",
            "Epoch 2/8\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 172ms/step - accuracy: 0.9067 - loss: 0.2282 - val_accuracy: 0.8918 - val_loss: 0.2870\n",
            "Epoch 3/8\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 184ms/step - accuracy: 0.9481 - loss: 0.1410 - val_accuracy: 0.8929 - val_loss: 0.3143\n",
            "Epoch 4/8\n",
            "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 171ms/step - accuracy: 0.9648 - loss: 0.1011 - val_accuracy: 0.8950 - val_loss: 0.3847\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step\n",
            "BiLSTM -> Acc: 0.8670, Precision: 0.6277, Recall: 0.8626, F1: 0.7266\n",
            "\n",
            "=== RESULTS SUMMARY ==\n",
            "                   model  accuracy  precision   recall       f1\n",
            "         LinearSVC-TFIDF  0.907752   0.782609 0.761099 0.771704\n",
            "LogisticRegression-TFIDF  0.898224   0.743852 0.767442 0.755463\n",
            "          MLP-AverageEmb  0.888696   0.691489 0.824524 0.752170\n",
            "                   CNN1D  0.886964   0.695572 0.797040 0.742857\n",
            "                  BiLSTM  0.867042   0.627692 0.862579 0.726625\n"
          ]
        }
      ]
    }
  ]
}