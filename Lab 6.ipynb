{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPueyCv71O+lbaZjeduazIx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AKHIRANANDHINI/NLP/blob/main/Lab%206.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import zipfile # Import the zipfile module\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Correctly read 'Tweets.csv' from the zip file\n",
        "with zipfile.ZipFile(\"/content/archive (1).zip\") as z:\n",
        "    with z.open(\"Tweets.csv\") as f:\n",
        "        df = pd.read_csv(f)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = [w for w in text.split() if w not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df['clean'] = df['text'].astype(str).apply(clean_text)\n",
        "\n",
        "X = df['clean']\n",
        "y = df['airline_sentiment'] # Corrected column name to 'airline_sentiment'\n",
        "\n",
        "# Convert 'target' column to numerical representation if it's categorical\n",
        "# Assuming 'positive' is 1 and 'negative' is 0 for binary classification\n",
        "df_binary = df[df['airline_sentiment'].isin(['positive', 'negative'])].copy()\n",
        "df_binary['target'] = df_binary['airline_sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "X = df_binary['clean'].values\n",
        "y = df_binary['target'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def run_ann(ngram_range):\n",
        "    vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_features=10000)\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = vectorizer.transform(X_test)\n",
        "    model = Sequential([\n",
        "        Dense(256, activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(X_train_tfidf.toarray(), y_train, epochs=5, batch_size=64, verbose=0,\n",
        "              validation_data=(X_test_tfidf.toarray(), y_test))\n",
        "    train_acc = model.evaluate(X_train_tfidf.toarray(), y_train, verbose=0)[1]\n",
        "    test_acc = model.evaluate(X_test_tfidf.toarray(), y_test, verbose=0)[1]\n",
        "    return train_acc, test_acc\n",
        "\n",
        "print(\"\\n=== ANN with TF-IDF ===\")\n",
        "uni_acc = run_ann((1,1))\n",
        "print(\"Unigram -> Train: %.4f, Test: %.4f\" % uni_acc)\n",
        "bi_acc  = run_ann((1,2))\n",
        "print(\"Unigram+Bigram -> Train: %.4f, Test: %.4f\" % bi_acc)\n",
        "tri_acc = run_ann((1,3))\n",
        "print(\"Unigram+Bigram+Trigram -> Train: %.4f, Test: %.4f\" % tri_acc)\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQ_LEN = 100\n",
        "EMB_DIM = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=MAX_SEQ_LEN)\n",
        "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=MAX_SEQ_LEN)\n",
        "\n",
        "def run_lstm():\n",
        "    model = Sequential([\n",
        "        Embedding(MAX_NB_WORDS, EMB_DIM), # Removed input_length\n",
        "        SpatialDropout1D(0.2),\n",
        "        LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(X_train_seq, y_train, epochs=5, batch_size=64,\n",
        "              validation_data=(X_test_seq, y_test), verbose=0)\n",
        "    train_acc = model.evaluate(X_train_seq, y_train, verbose=0)[1]\n",
        "    test_acc = model.evaluate(X_test_seq, y_test, verbose=0)[1]\n",
        "    return train_acc, test_acc\n",
        "\n",
        "print(\"\\n=== LSTM with Embeddings ===\")\n",
        "lstm_acc = run_lstm()\n",
        "print(\"LSTM -> Train: %.4f, Test: %.4f\" % lstm_acc)\n",
        "\n",
        "print(\"\\n=== Analysis ===\")\n",
        "print(\"Bigrams usually outperform unigrams because phrases like 'fire alarm' or 'flood warning' carry more meaning than single words.\")\n",
        "print(\"Trigrams often add sparsity and risk of overfitting.\")\n",
        "print(\"LSTM already learns sequential dependencies, so explicit bigrams/trigrams help less.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV-Gl96WGSNf",
        "outputId": "7601c91c-34d1-4b67-cbd4-dc4b0eb14dab"
      },
      "execution_count": 19,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== ANN with TF-IDF ===\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram -> Train: 0.9984, Test: 0.9099\n",
            "Unigram+Bigram -> Train: 0.9984, Test: 0.9104\n",
            "Unigram+Bigram+Trigram -> Train: 0.9979, Test: 0.9168\n",
            "\n",
            "=== LSTM with Embeddings ===\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM -> Train: 0.9925, Test: 0.9125\n",
            "\n",
            "=== Analysis ===\n",
            "Bigrams usually outperform unigrams because phrases like 'fire alarm' or 'flood warning' carry more meaning than single words.\n",
            "Trigrams often add sparsity and risk of overfitting.\n",
            "LSTM already learns sequential dependencies, so explicit bigrams/trigrams help less.\n"
          ]
        }
      ]
    }
  ]
}